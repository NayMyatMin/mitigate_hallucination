2025-03-18 00:28:10 - __main__ - INFO - Model: llama3.1-8b-instruct
2025-03-18 00:28:10 - __main__ - INFO - Dataset: mixed
2025-03-18 00:28:10 - __main__ - INFO - Output directory: outputs/llama3.1-8b-instruct_mixed_20250318_002810
2025-03-18 00:28:10 - __main__ - INFO - Entropy weight: 0.1
2025-03-18 00:28:10 - __main__ - INFO - Loading model and tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [00:24<01:12, 24.20s/it]Downloading shards:  50%|█████     | 2/4 [00:48<00:48, 24.16s/it]Downloading shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.93s/it]Downloading shards: 100%|██████████| 4/4 [01:17<00:00, 16.67s/it]Downloading shards: 100%|██████████| 4/4 [01:17<00:00, 19.40s/it]
2025-03-18 00:29:35 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:24,  8.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:18,  9.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:27<00:09,  9.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  5.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.01s/it]
2025-03-18 00:30:04 - __main__ - INFO - Preparing model for LoRA training...
2025-03-18 00:30:05 - __main__ - INFO - Preparing dataset...
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
Map:   0%|          | 0/300 [00:00<?, ? examples/s]Map:   0%|          | 0/300 [00:00<?, ? examples/s]
Error loading factual dataset: 'correct_answers'
Error loading hallucinations dataset: Dataset 'vectara/hallucination-leaderboard' doesn't exist on the Hub or cannot be accessed.
Loaded citations dataset:
  - Train size: 300
  - Eval size: 100
Traceback (most recent call last):
  File "/common/home/users/m/myatmin.nay.2022/mitigate_hallucination/train.py", line 341, in <module>
    main() 
    ^^^^^^
  File "/common/home/users/m/myatmin.nay.2022/mitigate_hallucination/train.py", line 278, in main
    dataset = prepare_dataset_for_training(args.dataset_type, tokenizer)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/users/m/myatmin.nay.2022/mitigate_hallucination/train.py", line 203, in prepare_dataset_for_training
    mixed_train = mix_datasets(train_datasets, HALLUCINATION_CONFIG["data_mixing_ratio"])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/users/m/myatmin.nay.2022/mitigate_hallucination/utils/data_utils.py", line 345, in mix_datasets
    max_ratio_key = max(dataset_samples.keys(), key=lambda k: normalized_ratios[k])
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: max() iterable argument is empty
